{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deploying Driverless AI Models to Production \u00b6 This documentation lists some of the scenarios to deploy Driverless AI models to production and provides guidlines to create deployment templates for the same. The final model from a Driverless AI experiment can be exported as either a MOJO scoring pipeline or a Python scoring pipeline . The Mojo scoring pipeline comes with a pipline.mojo file with a Java or C++ runtime. This can be deployed in any environment that supports Java or C++. The Python scoring pipeline comes with the scoring whl file for deployment purposes. Below, is a list of the deployment scenerios for Driverless AI pipelines for Real-time, Batch or Stream scoring. graph LR; A[\"Realtime Scoring (single or multi-rows)\"]-->AA[DAI MOJO Pipeline with Java Runtime]; A-->AB[DAI MOJO Pipeline with C++ Runtime]; A-->AC[DAI PYTHON Scoring Pipeline]; AA-->AAA[\"As REST Server\"] AA-->AAB[\"As AWS Lambda\"] AA-->AAC[\"As GCP Cloud Run\"] AA-->AAD[\"As AzureML\"] AA-->AAE[\"As library\"] AA-->AAF[\"As Apache NiFi \"] AA-->AAG[\"As Apache Flink\"] AB-->ABA[\"As library\"] AB-->ABB[\"As Apache NiFi \"] AC-->ACA[\"As REST Server\"] AC-->ACB[\"As Apache NiFi\"] click AAA \"./local-rest-scorer\" click AAB \"./aws_lambda_scorer\" click AAC \"./gcp\" click AAF \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-nifi\" click AAG \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-flink\" graph LR; A[\"Batch Scoring\"]-->AA[DAI MOJO Pipeline with Java Runtime]; A-->AB[DAI MOJO Pipeline with C++ Runtime]; A-->AC[DAI PYTHON Scoring Pipeline]; AA-->AAA[\"As Apache Spark batch\"] AA-->AAB[\"As library\"] AA-->AAC[\"As Hive UDF\"] AA-->AAD[\"As DB scorer\"] AA-->AAE[\"As Apache NiFi\"] AA-->AAF[\"As Apache Flink\"] AA-->AAG[\"As Snowflake Workflow\"] AB-->ABA[\"As library\"] AB-->ABB[\"As Apache NiFi\"] AC-->ACA[\"As library\"] AC-->ACB[\"As Apache NiFi\"] click AAA \"http://docs.h2o.ai/sparkling-water/3.0/latest-stable/doc/deployment/load_mojo_pipeline.html#loading-and-score-the-mojo\" click AAD \"./sql-jdbc-scorer\" click AAE \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-nifi\" click AAF \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-flink\" click AAG \"https://www.youtube.com/watch?v=EMpGVer01WE\" graph LR; A[\"Stream Scoring\"]-->AA[DAI MOJO Pipeline with Java Runtime]; AA-->AAA[\"As Apache Spark Stream\"] AA-->AAB[\"As Task Queue \"] AA-->AAC[\"As Active MQ\"] AA-->AAD[\"As Kafka Topic\"] click AAD \"https://github.com/h2oai/dai-deployment-examples/blob/master/mojo-flink/daimojo-flink-kafka.md\"","title":"Overview"},{"location":"#deploying-driverless-ai-models-to-production","text":"This documentation lists some of the scenarios to deploy Driverless AI models to production and provides guidlines to create deployment templates for the same. The final model from a Driverless AI experiment can be exported as either a MOJO scoring pipeline or a Python scoring pipeline . The Mojo scoring pipeline comes with a pipline.mojo file with a Java or C++ runtime. This can be deployed in any environment that supports Java or C++. The Python scoring pipeline comes with the scoring whl file for deployment purposes. Below, is a list of the deployment scenerios for Driverless AI pipelines for Real-time, Batch or Stream scoring. graph LR; A[\"Realtime Scoring (single or multi-rows)\"]-->AA[DAI MOJO Pipeline with Java Runtime]; A-->AB[DAI MOJO Pipeline with C++ Runtime]; A-->AC[DAI PYTHON Scoring Pipeline]; AA-->AAA[\"As REST Server\"] AA-->AAB[\"As AWS Lambda\"] AA-->AAC[\"As GCP Cloud Run\"] AA-->AAD[\"As AzureML\"] AA-->AAE[\"As library\"] AA-->AAF[\"As Apache NiFi \"] AA-->AAG[\"As Apache Flink\"] AB-->ABA[\"As library\"] AB-->ABB[\"As Apache NiFi \"] AC-->ACA[\"As REST Server\"] AC-->ACB[\"As Apache NiFi\"] click AAA \"./local-rest-scorer\" click AAB \"./aws_lambda_scorer\" click AAC \"./gcp\" click AAF \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-nifi\" click AAG \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-flink\" graph LR; A[\"Batch Scoring\"]-->AA[DAI MOJO Pipeline with Java Runtime]; A-->AB[DAI MOJO Pipeline with C++ Runtime]; A-->AC[DAI PYTHON Scoring Pipeline]; AA-->AAA[\"As Apache Spark batch\"] AA-->AAB[\"As library\"] AA-->AAC[\"As Hive UDF\"] AA-->AAD[\"As DB scorer\"] AA-->AAE[\"As Apache NiFi\"] AA-->AAF[\"As Apache Flink\"] AA-->AAG[\"As Snowflake Workflow\"] AB-->ABA[\"As library\"] AB-->ABB[\"As Apache NiFi\"] AC-->ACA[\"As library\"] AC-->ACB[\"As Apache NiFi\"] click AAA \"http://docs.h2o.ai/sparkling-water/3.0/latest-stable/doc/deployment/load_mojo_pipeline.html#loading-and-score-the-mojo\" click AAD \"./sql-jdbc-scorer\" click AAE \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-nifi\" click AAF \"https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-flink\" click AAG \"https://www.youtube.com/watch?v=EMpGVer01WE\" graph LR; A[\"Stream Scoring\"]-->AA[DAI MOJO Pipeline with Java Runtime]; AA-->AAA[\"As Apache Spark Stream\"] AA-->AAB[\"As Task Queue \"] AA-->AAC[\"As Active MQ\"] AA-->AAD[\"As Kafka Topic\"] click AAD \"https://github.com/h2oai/dai-deployment-examples/blob/master/mojo-flink/daimojo-flink-kafka.md\"","title":"Deploying Driverless AI Models to Production"},{"location":"aws-sagemaker-hosted-scorer/","text":"Driverless AI Deployment Template for AWS Sagemaker Hosted Scorer \u00b6 This template scores realtime data using Driverless AI Mojo pipeline with Java Runtime and plugs into the AWS SageMaker workflow documented here It is a REST API which accepts one data point at a time for prediction in real-time in the hosted SageMaker environment. The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring, see deploy section for details. The versions of software used to create the template like mojo runtime are listed here . Overview \u00b6 Build Image \u00b6 Generation of the Docker image is plugged into the build process of this project. Run the following command in the root project directory to run the build process. ./gradlew :aws-sagemaker-hosted-scorer:jibDockerBuild Verify that the Docker image was created, and take note of the version created. docker images --format \"{{.Repository}} \\t {{.Tag}}\" | grep \"h2oai/sagemaker-hosted-scorer\" Optional: Test the build \u00b6 After building, run to test the produced Docker container locally like this: Step 1: Put a pipeline.mojo and valid license.sig into this directory (aws-sagemaker-hosted-scorer). Step 2: Start the docker instance. docker run \\ --rm \\ --init \\ -ti \\ -v `pwd`:/opt/ml/model \\ -p 8080:8080 \\ harbor.h2o.ai/opsh2oai/h2oai/sagemaker-hosted-scorer \\ serve Step 3: Use curl to send a JSON-formatted row to the scorer as shown in the details below. Deploy to SageMaker \u00b6 Create h2oai/sagemaker-hosted-scorer repository in Sagemaker for the scorer service image. Use the output of the command below to docker login : aws ecr get-login --region <region> --no-include-email Tag the scorer service image for loading into the h2oai/sagemaker-hosted-scorer repository. docker tag <IMAGE ID> <aws_account_id>.dkr.ecr.<region>.amazonaws.com/h2oai/sagemaker-hosted-scorer Then push the scorer service image to AWS ECR (Elastic Container Registry): docker push <aws_account_id>.dkr.ecr.<region>.amazonaws.com/h2oai/sagemaker-hosted-scorer Then create a model package with the pipeline file and the license key, and copy it to S3: tar cvf mojo.tar pipeline.mojo license.sig gzip mojo.tar aws s3 cp mojo.tar.gz s3://<your-bucket>/ Next create the appropriate model and endpoint on Sagemaker. Check that the endpoint is available with aws sagemaker list-endpoints . Examples \u00b6 There is the examples directory, which houses a sample python notebook, which utilizes the AWS Sagemaker SDK to deploy a Sagemaker model pythonically. See the notebook here For an example of an endpoint query being made via Python go here Details \u00b6 AWS Model Creation API \u00b6 https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html CreateEndpoint Environment DRIVERLESS_AI_LICENSE_KEY=base64key ModelDataURL=s3://blah/blah/model.tar.gz DRIVERLESS_AI_LICENSE_KEY environment variable must contain the base64-encoded key (optional if a license.sig isns't included in mojo.tar.gz) ModelDataURL must point to an S3 URL with a .tar.gz file of the MOJO artifact Docker container \u00b6 The docker container produced in this directory conforms to the specification described here: https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image Sagemaker starts the container with the following command: docker run image serve Our container consists of the following entrypoint: ENTRYPOINT [\"java\", \"-jar\", \"serve.jar\"] Example Predictions \u00b6 You can test the container locally with the following curl command: curl \\ -X POST \\ -H \"Content-Type: application/json\" \\ -d @test.json http://localhost:8080/invocations test.json: { \"fields\": [ \"field1\", \"field2\" ], \"includeFieldsInOutput\": [ \"field2\" ], \"rows\": [ [ \"value1\", \"value2\" ], [ \"value1\", \"value2\" ] ] }","title":"AWS Sagemaker Hosted Scorer"},{"location":"aws-sagemaker-hosted-scorer/#driverless-ai-deployment-template-for-aws-sagemaker-hosted-scorer","text":"This template scores realtime data using Driverless AI Mojo pipeline with Java Runtime and plugs into the AWS SageMaker workflow documented here It is a REST API which accepts one data point at a time for prediction in real-time in the hosted SageMaker environment. The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring, see deploy section for details. The versions of software used to create the template like mojo runtime are listed here .","title":"Driverless AI Deployment Template for AWS Sagemaker Hosted Scorer"},{"location":"aws-sagemaker-hosted-scorer/#overview","text":"","title":"Overview"},{"location":"aws-sagemaker-hosted-scorer/#build-image","text":"Generation of the Docker image is plugged into the build process of this project. Run the following command in the root project directory to run the build process. ./gradlew :aws-sagemaker-hosted-scorer:jibDockerBuild Verify that the Docker image was created, and take note of the version created. docker images --format \"{{.Repository}} \\t {{.Tag}}\" | grep \"h2oai/sagemaker-hosted-scorer\"","title":"Build Image"},{"location":"aws-sagemaker-hosted-scorer/#optional-test-the-build","text":"After building, run to test the produced Docker container locally like this: Step 1: Put a pipeline.mojo and valid license.sig into this directory (aws-sagemaker-hosted-scorer). Step 2: Start the docker instance. docker run \\ --rm \\ --init \\ -ti \\ -v `pwd`:/opt/ml/model \\ -p 8080:8080 \\ harbor.h2o.ai/opsh2oai/h2oai/sagemaker-hosted-scorer \\ serve Step 3: Use curl to send a JSON-formatted row to the scorer as shown in the details below.","title":"Optional: Test the build"},{"location":"aws-sagemaker-hosted-scorer/#deploy-to-sagemaker","text":"Create h2oai/sagemaker-hosted-scorer repository in Sagemaker for the scorer service image. Use the output of the command below to docker login : aws ecr get-login --region <region> --no-include-email Tag the scorer service image for loading into the h2oai/sagemaker-hosted-scorer repository. docker tag <IMAGE ID> <aws_account_id>.dkr.ecr.<region>.amazonaws.com/h2oai/sagemaker-hosted-scorer Then push the scorer service image to AWS ECR (Elastic Container Registry): docker push <aws_account_id>.dkr.ecr.<region>.amazonaws.com/h2oai/sagemaker-hosted-scorer Then create a model package with the pipeline file and the license key, and copy it to S3: tar cvf mojo.tar pipeline.mojo license.sig gzip mojo.tar aws s3 cp mojo.tar.gz s3://<your-bucket>/ Next create the appropriate model and endpoint on Sagemaker. Check that the endpoint is available with aws sagemaker list-endpoints .","title":"Deploy to SageMaker"},{"location":"aws-sagemaker-hosted-scorer/#examples","text":"There is the examples directory, which houses a sample python notebook, which utilizes the AWS Sagemaker SDK to deploy a Sagemaker model pythonically. See the notebook here For an example of an endpoint query being made via Python go here","title":"Examples"},{"location":"aws-sagemaker-hosted-scorer/#details","text":"","title":"Details"},{"location":"aws-sagemaker-hosted-scorer/#aws-model-creation-api","text":"https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html CreateEndpoint Environment DRIVERLESS_AI_LICENSE_KEY=base64key ModelDataURL=s3://blah/blah/model.tar.gz DRIVERLESS_AI_LICENSE_KEY environment variable must contain the base64-encoded key (optional if a license.sig isns't included in mojo.tar.gz) ModelDataURL must point to an S3 URL with a .tar.gz file of the MOJO artifact","title":"AWS Model Creation API"},{"location":"aws-sagemaker-hosted-scorer/#docker-container","text":"The docker container produced in this directory conforms to the specification described here: https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image Sagemaker starts the container with the following command: docker run image serve Our container consists of the following entrypoint: ENTRYPOINT [\"java\", \"-jar\", \"serve.jar\"]","title":"Docker container"},{"location":"aws-sagemaker-hosted-scorer/#example-predictions","text":"You can test the container locally with the following curl command: curl \\ -X POST \\ -H \"Content-Type: application/json\" \\ -d @test.json http://localhost:8080/invocations test.json: { \"fields\": [ \"field1\", \"field2\" ], \"includeFieldsInOutput\": [ \"field2\" ], \"rows\": [ [ \"value1\", \"value2\" ], [ \"value1\", \"value2\" ] ] }","title":"Example Predictions"},{"location":"aws_lambda_scorer/","text":"Driverless AI MOJO pipeline Deployment Template for AWS Lambda \u00b6 This template contains scorer implementation for Driverless AI Model MOJO pipiline with Java runtime on AWS Lambda and can be used to do Real-time scoring on single or multiple rows. The structure is as follows: Source of the generic lambda implementation in: lambda-template Parameterized terraform files for pushing the lambda to AWS in: terraform-recipe The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring. The versions of software used to create the template like mojo runtime are listed here . Building the lambda-template \u00b6 The code of the AWS Lambda scorer is a gradle project build as usual by ./gradlew build . The build result is a Zip archive lambda-template/build/distributions/lambda-template.zip containing a general Mojo scorer that can be directly pushed to AWS. The Scorer relies on the following environment variables: DEPLOYMENT_S3_BUCKET_NAME : Name of the AWS S3 bucket storing the Mojo file. MOJO_S3_OBJECT_KEY : Key of Driverless AI model Mojo file (pipeline.mojo) AWS S3 object. DRIVERLESS_AI_LICENSE_KEY : The Driverless license key. Pushing to AWS Using Terraform \u00b6 This deployment template is meant to be used by Driverless AI backend directly, not by hand. The following describes step necessary to push the lambda by hand, e.g., for testing purposes. One-off Setup \u00b6 Install terraform 0.11.10 following steps in: https://www.terraform.io/downloads.html. Initialize terraform by running terraform init in the terraform-recipe folder. This will download all necessary Terraform plugins, e.g., the AWS one. Pushing Lambda to AWS \u00b6 The Terraform recipe in terraform-recipe relies on a few variables you need to provide either by hand, from command line, or by setting corresponding environmental variables (using the prefix TF_VAR_ ): access_key : The access key to your AWS account. secret_key : The secret key to your AWS account. region : AWS region to push to (optional, defaults to us-east-1 ). lambda_id : Id of the resulting AWS lambda. Keep that unique as it is also used to store other fragments, e.g., the Mojo file in S3. lambda_zip_path : Local path to the actual lambda scorer distribution, see above (optional, defaults to the relative path to the build Zip archive above). mojo_path : Local path to the mojo file to be pushed to S3. You may get one, e.g., by running Driverless AI on test/data/iris.csv and asking it to create and download the Mojo scoring pipeline in the UI. license_key : Driverless AI license key. bucket_name : Name of AWS S3 bucket to store the mojo to so that the lambda can access and load it. Note that the bucket has to already exist and the effective AWS account has to have write access to it. Once all the non-optional variables are set, the following command will push the lambda (or update any changes thereof): terraform apply . Upon successful push, Terraform will output the URL of the lambda endpoint and the corresponding api_key . Note that the recipe sets up AWS API Gateway proxy, see api_gateway.tf . Look for base_url and api_key in the output. Outputs: api_key = DXQtiCbqEY6xjXWP1MMCu4nkDTwRgfdX2qZoKm3e base_url = https://mslmi91tni.execute-api.us-east-1.amazonaws.com/test Example Prediction \u00b6 To test the endpoint, send a request to this URL appended by score and include the api_key in the request header x-api-key , e.g., as follows. $ curl \\ -X POST \\ -H \"x-api-key: DXQtiCbqEY6xjXWP1MMCu4nkDTwRgfdX2qZoKm3e\" \\ -d @test.json https://mslmi91tni.execute-api.us-east-1.amazonaws.com/test/score This expects a file test.json with the actual scoring request payload. If you are using the mojo trained in test/data/iris.csv as suggested above, you should be able to use the following json payload: { \"fields\": [ \"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\" ], \"includeFieldsInOutput\": [ \"sepal_len\" ], \"rows\": [ [ \"1.0\", \"1.0\", \"2.2\", \"3.5\" ], [ \"3.0\", \"10.0\", \"2.2\", \"3.5\" ], [ \"4.0\", \"100.0\", \"2.2\", \"3.5\" ] ] } The expected response should follow this structure: { \"id\": \"a12e7390-b8ac-406a-ade9-0d5ea4b63ea9\", \"fields\": [ \"sepal_len\", \"class.Iris-setosa\", \"class.Iris-versicolor\", \"class.Iris-virginica\" ], \"score\": [ [ \"1.0\", \"0.6240277982943945\", \"0.045458571508101536\", \"0.330513630197504\" ], [ \"3.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ], [ \"4.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ] ] } Note that including the fields in the response can be disabled by setting noFieldNamesInOutput to true in the input request.","title":"AWS Lambda Scorer"},{"location":"aws_lambda_scorer/#driverless-ai-mojo-pipeline-deployment-template-for-aws-lambda","text":"This template contains scorer implementation for Driverless AI Model MOJO pipiline with Java runtime on AWS Lambda and can be used to do Real-time scoring on single or multiple rows. The structure is as follows: Source of the generic lambda implementation in: lambda-template Parameterized terraform files for pushing the lambda to AWS in: terraform-recipe The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring. The versions of software used to create the template like mojo runtime are listed here .","title":"Driverless AI MOJO pipeline Deployment Template for AWS Lambda"},{"location":"aws_lambda_scorer/#building-the-lambda-template","text":"The code of the AWS Lambda scorer is a gradle project build as usual by ./gradlew build . The build result is a Zip archive lambda-template/build/distributions/lambda-template.zip containing a general Mojo scorer that can be directly pushed to AWS. The Scorer relies on the following environment variables: DEPLOYMENT_S3_BUCKET_NAME : Name of the AWS S3 bucket storing the Mojo file. MOJO_S3_OBJECT_KEY : Key of Driverless AI model Mojo file (pipeline.mojo) AWS S3 object. DRIVERLESS_AI_LICENSE_KEY : The Driverless license key.","title":"Building the lambda-template"},{"location":"aws_lambda_scorer/#pushing-to-aws-using-terraform","text":"This deployment template is meant to be used by Driverless AI backend directly, not by hand. The following describes step necessary to push the lambda by hand, e.g., for testing purposes.","title":"Pushing to AWS Using Terraform"},{"location":"aws_lambda_scorer/#one-off-setup","text":"Install terraform 0.11.10 following steps in: https://www.terraform.io/downloads.html. Initialize terraform by running terraform init in the terraform-recipe folder. This will download all necessary Terraform plugins, e.g., the AWS one.","title":"One-off Setup"},{"location":"aws_lambda_scorer/#pushing-lambda-to-aws","text":"The Terraform recipe in terraform-recipe relies on a few variables you need to provide either by hand, from command line, or by setting corresponding environmental variables (using the prefix TF_VAR_ ): access_key : The access key to your AWS account. secret_key : The secret key to your AWS account. region : AWS region to push to (optional, defaults to us-east-1 ). lambda_id : Id of the resulting AWS lambda. Keep that unique as it is also used to store other fragments, e.g., the Mojo file in S3. lambda_zip_path : Local path to the actual lambda scorer distribution, see above (optional, defaults to the relative path to the build Zip archive above). mojo_path : Local path to the mojo file to be pushed to S3. You may get one, e.g., by running Driverless AI on test/data/iris.csv and asking it to create and download the Mojo scoring pipeline in the UI. license_key : Driverless AI license key. bucket_name : Name of AWS S3 bucket to store the mojo to so that the lambda can access and load it. Note that the bucket has to already exist and the effective AWS account has to have write access to it. Once all the non-optional variables are set, the following command will push the lambda (or update any changes thereof): terraform apply . Upon successful push, Terraform will output the URL of the lambda endpoint and the corresponding api_key . Note that the recipe sets up AWS API Gateway proxy, see api_gateway.tf . Look for base_url and api_key in the output. Outputs: api_key = DXQtiCbqEY6xjXWP1MMCu4nkDTwRgfdX2qZoKm3e base_url = https://mslmi91tni.execute-api.us-east-1.amazonaws.com/test","title":"Pushing Lambda to AWS"},{"location":"aws_lambda_scorer/#example-prediction","text":"To test the endpoint, send a request to this URL appended by score and include the api_key in the request header x-api-key , e.g., as follows. $ curl \\ -X POST \\ -H \"x-api-key: DXQtiCbqEY6xjXWP1MMCu4nkDTwRgfdX2qZoKm3e\" \\ -d @test.json https://mslmi91tni.execute-api.us-east-1.amazonaws.com/test/score This expects a file test.json with the actual scoring request payload. If you are using the mojo trained in test/data/iris.csv as suggested above, you should be able to use the following json payload: { \"fields\": [ \"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\" ], \"includeFieldsInOutput\": [ \"sepal_len\" ], \"rows\": [ [ \"1.0\", \"1.0\", \"2.2\", \"3.5\" ], [ \"3.0\", \"10.0\", \"2.2\", \"3.5\" ], [ \"4.0\", \"100.0\", \"2.2\", \"3.5\" ] ] } The expected response should follow this structure: { \"id\": \"a12e7390-b8ac-406a-ade9-0d5ea4b63ea9\", \"fields\": [ \"sepal_len\", \"class.Iris-setosa\", \"class.Iris-versicolor\", \"class.Iris-virginica\" ], \"score\": [ [ \"1.0\", \"0.6240277982943945\", \"0.045458571508101536\", \"0.330513630197504\" ], [ \"3.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ], [ \"4.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ] ] } Note that including the fields in the response can be disabled by setting noFieldNamesInOutput to true in the input request.","title":"Example Prediction"},{"location":"dai-mojo-db/","text":"Database Scorer \u00b6 Some customers need to select rows from a Database, score and write back the predictions.Using DAI and then this program enables them to quickly build and use models. This scorer contains scoring implementation for Driverless AI Model MOJO pipiline with Java runtime on various different databases and can be used to do Real-time scoring on single or multiple rows. Database Scorer Supports following databases 1. Postgres DB 2. Microsoft SQL Server 12 3. TerraData 4. RedShift 5. Oracle 6. DB2 7. Azure Data Warehouse 8. Snowflake 9. ElasticSearch Downloading Instructions \u00b6 You can download the database scorer from the downloads page Deployment \u00b6 You must have a DAI license to score, so add the license (via command line) in one of the standard ways, as a parameter, as a environment vailable for example: Dai.h2o.mojos.runtime.license.file=/Driverless-AI/license/license.sig ``` The license can also be passed: Environment variable DRIVERLESS_AI_LICENSE_FILE : A location of file with a license. Environment variable DRIVERLESS_AI_LICENSE_KEY : A license key. System properties of JVM (-D option) 'ai.h2o.mojos.runtime.license.file' : A location of System properties of JVM (-D option) 'ai.h2o.mojos.runtime.license.key' : A license key. Classpath: The license is loaded from resource called '/license.sig' The default resource name can be changed via system property 'ai.h2o.mojos.runtime.license.filename' ``` To run the database scorer, you can directly run the executable jar: java -Dpropertiesfilename={PATH_TO_PROPERTIES_FILE} -jar build/libs/dai-mojo-db-{YOUR_CURRENT_VERSION}.jar Properties Files \u00b6 Using the flag -Dpropertiesfilename= multiple configuration files can be created for different models or databases, just specify the properties file to use at runtime. Sample Properties File \u00b6 ModelName=pipeline.mojo SQLConnectionString=jdbc:postgresql://192.168.1.171:5432/LendingClub SQLUser=postgres SQLPassword=aDJvaDJvCg== SQLPrompt= SQLKey=id SQLPrediction= SQLSelect=select id, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mths, pub_rec, revol_bal, revol_util, total_acc from \"import\".loanstats4 SQLWrite=update \"import\".loanstats4 set where id= The above will score using pipeline.mojo and use the key 'id' to update the column name from the mojo into the table, to bad_loan.0 and bad_loan.1 must be in the table schema. If SQLPrediction was set to ModelPrediction (for example) then bad_loan.0 would be written to the table column ModelPrediction. Usage \u00b6 java -Dpropertiesfilename=DAI-Mojo-DB/properties/DAIMojoRunner_DB.properties-Snowflake -jar DAI-Mojo-DB/build/libs/dai-mojo-db-2.30.jar Runtime Flags \u00b6 While the Database and Mojo configuration is in the properties file, the command line does have some flags -Dverbose=true | false{default} verbosity flag -Doverride=true | false{default} bypass check if running multithreads on a table without an index, as the table would be in a random order. -Dwait=true | false: {default} causes the program to wait for Enter once the model and threads are ready, used for demos. -Dstats=true | false{default} writes performance statistics at completion. -Dinspect=true | false{default} prints model details and suggest SQLSelect based on model as well as runtime java memory settings. -Dcapacity= Number of available Cores+33%{default} Size of buffer queue. -Dthreads= Number of available Cores{default} -Dpropertiesfilename=DAIMojoRunner_DB.properties {default} allows multiple properties files to be created for different models and databases Example: java -Derrors=true -Dverbose=true -Dthreads=2 -Dcapacity=4000 -Xms2g -Xmx10g -Dpropertiesfilename={PATH_TO_PROPERTIES_FILE} -jar build/libs/dai-mojo-db-{YOUR_CURRENT_VERSION}.jar Deployment Tricks \u00b6 Its important to also have a key for each row being scored, this is because the scorer will execute in parallel (based on the -Dthreads= parameter) so updating the databases will be faster if a unique key is used for each row (use a customer id or even the row number) for the SQLKey statement. The option -Dinsepect=true can be used to generate the SQL based on the model, this helps when a large number of database columns are required. This example assumes the model is called pipeline.mojo but could be change using the properties file. ``` input \u00b6 java -Dinspect=true -jar DAIMojoRunner_DB.jar output \u00b6 Details of Model: pipeline.mojo UUID: b2fce6c1-6ddc-4c78-8355-f437945a613c Input Features 0 = Name: loan_amnt Type: Float32 1 = Name: term Type: Str 2 = Name: int_rate Type: Float32 3 = Name: installment Type: Float32 4 = Name: emp_length Type: Float32 5 = Name: home_ownership Type: Str 6 = Name: annual_inc Type: Float32 7 = Name: verification_status Type: Str 8 = Name: addr_state Type: Str 9 = Name: dti Type: Float32 10 = Name: delinq_2yrs Type: Float32 11 = Name: inq_last_6mths Type: Float32 12 = Name: pub_rec Type: Float32 13 = Name: revol_bal Type: Float32 14 = Name: revol_util Type: Float32 15 = Name: total_acc Type: Float32 Output Features 0 = Name: bad_loan.0 Type: Float64 1 = Name: bad_loan.1 Type: Float64 Suggested configuration for properties file: select {add-table-index}, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mths, pub_rec, revol_bal, revol_util, total_acc from {add-table-name} update {add-table-name} set where {add-table-index}= ``` Note Change the values in {} above and manually test before using them in the program. The System has 16GB available physically. This program is using 0GB Consider adjusting -Xms and -Xmx to no more than 12GB The System has 8 Processors Handling Passwords \u00b6 Support For Encrypted Password \u00b6 Store the encrypted password in the DAIMojoRunner_DB.properties Encrypt the password from Linux command line: (type in the password): echo h2oh2o | base64 -i \u2013 Encrypt the password from Windows PowerShell (password passed as h2oh2o) Save the encrypted string to the SQLPassword parameter The password encryption can also be performed by setting the SQLPrompt to encrypt then run java -jar DAIMojoRunner_DB.jar you will be prompted to enter the JDBC password, it will then be encrypted so you can paste it into the properties file. Prompting for password \u00b6 Set SQLPrompt= Enables prompting for the password in the DAIMojoRunner_DB.properties Add the username to the SQLUser line make sure SQLPassword, is set SQLPassword=Set SQLPrompt=true Using Environment Variables \u00b6 If the execution environment has variables with the values for the parameters they can be used in the properties file. The setting EnvVariables=true must be in the properties file ideally the first line or before substitution is needed. Then use the environment variables like this: jdbc:sqlserver://'$ADW_HOST';databaseName='$ADW_DB';user='$ADW_USER';password='$ADW_PWD' Variable names start with $ and are delimited by ' For example: # IF THE ENVIRONMENTS ARE... ADW_DB=MyDatabase ADW_HOST=127.0.0.1:5033 ADW_PWD=%@#$^* ADW_USER=Admin # AT RUNTIME IT WOULD SUBSTITUTE TO... jdbc:sqlserver://127.0.0.1:5033;databaseName=MyDatabase;user=Admin;password=%@#$^* Using Windows Active Directory \u00b6 Some scoring environments want to use the credentials of the user executing the scoring rather than a service account. The score can use the AD for this, follow these steps: Add to the command line: -Djava.library.path=\\sqljdbc_6.0\\auth Add to the SQLConnectionString ;integratedSecurity=true For Example The command line would look like this: java -Xms4g -Xmx4g -Dlogging=true -Dpropertiesfilename=DAIMojoRunner_DB.properties-MSSQL-Auth -Dai.h2o.mojos.runtime.license.file={PATH_TO_LICENSE}/license.sig -Djava.library.path=C:\\sqljdbc_6.0\\enu\\auth\\x64 -cp {PATH_TO_JAR}/dai-mojo-db-2.30.jar ai.h2oai.mojos.db.daimojorunner_db.DAIMojoRunner_DB The SQLConnectionString in the properties file would look like this: SQLConnectionString=jdbc:sqlserver://192.168.1.173:1433;databaseName=LendingClub;integratedSecurity=true The SQLUser and SQLPassword would be commented out. Result \u00b6 The DB scorer stores the score/prediction for each row from the given dataset in a file called results.csv in the main directory. java -Derrors=true -Dverbose=true -Dthreads=2 -Dcapacity=4000 -Xms2g -Xmx10g -Dpropertiesfilename=properties/DAIMojoRunner_DB.properties-Snowflake -jar build/libs/dai-mojo-db-2.30.jar H2O DAI Mojo Database Scoring v2.30 Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry EnvVariables not found in properties file, setting to default Property EnvVariables = false Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry IgnoreEmptyStrings not found in properties file, setting to default Property IgnoreEmptyStrings = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry IgnoreChecks not found in properties file, setting to default Property IgnoreChecks = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry IgnoreSQLChecks not found in properties file, setting to default Property IgnoreSQLChecks = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry ForceConnection not found in properties file, setting to default Property ForceConnection = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry ForceConnectionClass not found in properties file, setting to default Property ForceConnectionClass = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property ModelName = Mojo/pipeline.mojo Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLConnectionString = jdbc:snowflake://h20_ai_partner.snowflakecomputing.com/?warehouse=DEMO_WH&db=LENDINGCLUB&schema=PUBLIC Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLUser = VIYENGAR Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLPassword = MjMwN0xlZ2hvcm4= Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLPrompt not found in properties file, setting to default Property SQLPrompt = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLSelect = select id, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mnths, pub_rec, revol_bal, revol_util, total_acc from \"LENDINGCLUB\"; Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLWrite = CSV Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLWriteCSVFilename = results.csv Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLWriteBatch not found in properties file, setting to default Property SQLWriteBatch = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLWriteBatchSize not found in properties file, setting to default Property SQLWriteBatchSize = 100000 Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLKey = id Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLPrediction not found in properties file, setting to default Property SQLPrediction = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLPredictionLabel not found in properties file, setting to default Property SQLPredictionLabel = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLFieldSeperator not found in properties file, setting to default Property SQLFieldSeperator = , Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry InternalFieldSeperator not found in properties file, setting to default Property InternalFieldSeperator = , Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLWriteFieldSeperator not found in properties file, setting to default Property SQLWriteFieldSeperator = , Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLStartScript = snowflake/SnowflakeStart.sql Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLEndScript = snowflake/SnowflakeUpdate.sql Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry Threshold not found in properties file, setting to default Property Threshold = Using model Mojo/pipeline.mojo TQ: 4000 2 Queue Capacity Starting 4000 Connection string used seperate SQL parameters Reader connected to database! {} Running SQL Script snowflake/SnowflakeStart.sql Script init completed Invoking SQL Script snowflake/SnowflakeStart.sql select count(*) from \"LENDINGCLUB\" COUNT(*) 39029 delete from \"RESULTS\" select count(*) from \"RESULTS\" COUNT(*) 0 SQL Script completed Reading ResultSet for select id, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mnths, pub_rec, revol_bal, revol_util, total_acc from \"LENDINGCLUB\"; ID, LOAN_AMNT, TERM, INT_RATE, INSTALLMENT, EMP_LENGTH, HOME_OWNERSHIP, ANNUAL_INC, VERIFICATION_STATUS, ADDR_STATE, DTI, DELINQ_2YRS, INQ_LAST_6MNTHS, PUB_REC, REVOL_BAL, REVOL_UTIL, TOTAL_ACC RAW ResultSet: 1077501 RAW ResultSet: 5000 RAW ResultSet: 36 months RAW ResultSet: 10.65 RAW ResultSet: 162.87 RAW ResultSet: 10 RAW ResultSet: RENT RAW ResultSet: 24000 ... Script init completed Invoking SQL Script snowflake/SnowflakeUpdate.sql put file://results.csv @~/staged source target source_size target_size source_compression target_compression status encryption message results.csv results.csv.gz 1783130 814416 NONE GZIP UPLOADED ENCRYPTED copy into RESULTS from @~/staged file_format = ( type = CSV) remove @~/stages/results.csv.gz name result select count(*) from \"RESULTS\" COUNT(*) 39029 SQL Script completed Note Please create a results.csv file in the main directory (if it doesn't exist already). It is important for the execution of the above command to have a results.csv file in the main dir to save the data processed by the scorer as the code will not create the file itself if it doesn't exist .","title":"Database Scorer"},{"location":"dai-mojo-db/#database-scorer","text":"Some customers need to select rows from a Database, score and write back the predictions.Using DAI and then this program enables them to quickly build and use models. This scorer contains scoring implementation for Driverless AI Model MOJO pipiline with Java runtime on various different databases and can be used to do Real-time scoring on single or multiple rows. Database Scorer Supports following databases 1. Postgres DB 2. Microsoft SQL Server 12 3. TerraData 4. RedShift 5. Oracle 6. DB2 7. Azure Data Warehouse 8. Snowflake 9. ElasticSearch","title":"Database Scorer"},{"location":"dai-mojo-db/#downloading-instructions","text":"You can download the database scorer from the downloads page","title":"Downloading Instructions"},{"location":"dai-mojo-db/#deployment","text":"You must have a DAI license to score, so add the license (via command line) in one of the standard ways, as a parameter, as a environment vailable for example: Dai.h2o.mojos.runtime.license.file=/Driverless-AI/license/license.sig ``` The license can also be passed: Environment variable DRIVERLESS_AI_LICENSE_FILE : A location of file with a license. Environment variable DRIVERLESS_AI_LICENSE_KEY : A license key. System properties of JVM (-D option) 'ai.h2o.mojos.runtime.license.file' : A location of System properties of JVM (-D option) 'ai.h2o.mojos.runtime.license.key' : A license key. Classpath: The license is loaded from resource called '/license.sig' The default resource name can be changed via system property 'ai.h2o.mojos.runtime.license.filename' ``` To run the database scorer, you can directly run the executable jar: java -Dpropertiesfilename={PATH_TO_PROPERTIES_FILE} -jar build/libs/dai-mojo-db-{YOUR_CURRENT_VERSION}.jar","title":"Deployment"},{"location":"dai-mojo-db/#properties-files","text":"Using the flag -Dpropertiesfilename= multiple configuration files can be created for different models or databases, just specify the properties file to use at runtime.","title":"Properties Files"},{"location":"dai-mojo-db/#sample-properties-file","text":"ModelName=pipeline.mojo SQLConnectionString=jdbc:postgresql://192.168.1.171:5432/LendingClub SQLUser=postgres SQLPassword=aDJvaDJvCg== SQLPrompt= SQLKey=id SQLPrediction= SQLSelect=select id, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mths, pub_rec, revol_bal, revol_util, total_acc from \"import\".loanstats4 SQLWrite=update \"import\".loanstats4 set where id= The above will score using pipeline.mojo and use the key 'id' to update the column name from the mojo into the table, to bad_loan.0 and bad_loan.1 must be in the table schema. If SQLPrediction was set to ModelPrediction (for example) then bad_loan.0 would be written to the table column ModelPrediction.","title":"Sample Properties File"},{"location":"dai-mojo-db/#usage","text":"java -Dpropertiesfilename=DAI-Mojo-DB/properties/DAIMojoRunner_DB.properties-Snowflake -jar DAI-Mojo-DB/build/libs/dai-mojo-db-2.30.jar","title":"Usage"},{"location":"dai-mojo-db/#runtime-flags","text":"While the Database and Mojo configuration is in the properties file, the command line does have some flags -Dverbose=true | false{default} verbosity flag -Doverride=true | false{default} bypass check if running multithreads on a table without an index, as the table would be in a random order. -Dwait=true | false: {default} causes the program to wait for Enter once the model and threads are ready, used for demos. -Dstats=true | false{default} writes performance statistics at completion. -Dinspect=true | false{default} prints model details and suggest SQLSelect based on model as well as runtime java memory settings. -Dcapacity= Number of available Cores+33%{default} Size of buffer queue. -Dthreads= Number of available Cores{default} -Dpropertiesfilename=DAIMojoRunner_DB.properties {default} allows multiple properties files to be created for different models and databases Example: java -Derrors=true -Dverbose=true -Dthreads=2 -Dcapacity=4000 -Xms2g -Xmx10g -Dpropertiesfilename={PATH_TO_PROPERTIES_FILE} -jar build/libs/dai-mojo-db-{YOUR_CURRENT_VERSION}.jar","title":"Runtime Flags"},{"location":"dai-mojo-db/#deployment-tricks","text":"Its important to also have a key for each row being scored, this is because the scorer will execute in parallel (based on the -Dthreads= parameter) so updating the databases will be faster if a unique key is used for each row (use a customer id or even the row number) for the SQLKey statement. The option -Dinsepect=true can be used to generate the SQL based on the model, this helps when a large number of database columns are required. This example assumes the model is called pipeline.mojo but could be change using the properties file. ```","title":"Deployment Tricks"},{"location":"dai-mojo-db/#input","text":"java -Dinspect=true -jar DAIMojoRunner_DB.jar","title":"input"},{"location":"dai-mojo-db/#output","text":"Details of Model: pipeline.mojo UUID: b2fce6c1-6ddc-4c78-8355-f437945a613c Input Features 0 = Name: loan_amnt Type: Float32 1 = Name: term Type: Str 2 = Name: int_rate Type: Float32 3 = Name: installment Type: Float32 4 = Name: emp_length Type: Float32 5 = Name: home_ownership Type: Str 6 = Name: annual_inc Type: Float32 7 = Name: verification_status Type: Str 8 = Name: addr_state Type: Str 9 = Name: dti Type: Float32 10 = Name: delinq_2yrs Type: Float32 11 = Name: inq_last_6mths Type: Float32 12 = Name: pub_rec Type: Float32 13 = Name: revol_bal Type: Float32 14 = Name: revol_util Type: Float32 15 = Name: total_acc Type: Float32 Output Features 0 = Name: bad_loan.0 Type: Float64 1 = Name: bad_loan.1 Type: Float64 Suggested configuration for properties file: select {add-table-index}, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mths, pub_rec, revol_bal, revol_util, total_acc from {add-table-name} update {add-table-name} set where {add-table-index}= ``` Note Change the values in {} above and manually test before using them in the program. The System has 16GB available physically. This program is using 0GB Consider adjusting -Xms and -Xmx to no more than 12GB The System has 8 Processors","title":"output"},{"location":"dai-mojo-db/#handling-passwords","text":"","title":"Handling Passwords"},{"location":"dai-mojo-db/#support-for-encrypted-password","text":"Store the encrypted password in the DAIMojoRunner_DB.properties Encrypt the password from Linux command line: (type in the password): echo h2oh2o | base64 -i \u2013 Encrypt the password from Windows PowerShell (password passed as h2oh2o) Save the encrypted string to the SQLPassword parameter The password encryption can also be performed by setting the SQLPrompt to encrypt then run java -jar DAIMojoRunner_DB.jar you will be prompted to enter the JDBC password, it will then be encrypted so you can paste it into the properties file.","title":"Support For Encrypted Password"},{"location":"dai-mojo-db/#prompting-for-password","text":"Set SQLPrompt= Enables prompting for the password in the DAIMojoRunner_DB.properties Add the username to the SQLUser line make sure SQLPassword, is set SQLPassword=Set SQLPrompt=true","title":"Prompting for password"},{"location":"dai-mojo-db/#using-environment-variables","text":"If the execution environment has variables with the values for the parameters they can be used in the properties file. The setting EnvVariables=true must be in the properties file ideally the first line or before substitution is needed. Then use the environment variables like this: jdbc:sqlserver://'$ADW_HOST';databaseName='$ADW_DB';user='$ADW_USER';password='$ADW_PWD' Variable names start with $ and are delimited by ' For example: # IF THE ENVIRONMENTS ARE... ADW_DB=MyDatabase ADW_HOST=127.0.0.1:5033 ADW_PWD=%@#$^* ADW_USER=Admin # AT RUNTIME IT WOULD SUBSTITUTE TO... jdbc:sqlserver://127.0.0.1:5033;databaseName=MyDatabase;user=Admin;password=%@#$^*","title":"Using Environment Variables"},{"location":"dai-mojo-db/#using-windows-active-directory","text":"Some scoring environments want to use the credentials of the user executing the scoring rather than a service account. The score can use the AD for this, follow these steps: Add to the command line: -Djava.library.path=\\sqljdbc_6.0\\auth Add to the SQLConnectionString ;integratedSecurity=true For Example The command line would look like this: java -Xms4g -Xmx4g -Dlogging=true -Dpropertiesfilename=DAIMojoRunner_DB.properties-MSSQL-Auth -Dai.h2o.mojos.runtime.license.file={PATH_TO_LICENSE}/license.sig -Djava.library.path=C:\\sqljdbc_6.0\\enu\\auth\\x64 -cp {PATH_TO_JAR}/dai-mojo-db-2.30.jar ai.h2oai.mojos.db.daimojorunner_db.DAIMojoRunner_DB The SQLConnectionString in the properties file would look like this: SQLConnectionString=jdbc:sqlserver://192.168.1.173:1433;databaseName=LendingClub;integratedSecurity=true The SQLUser and SQLPassword would be commented out.","title":"Using Windows Active Directory"},{"location":"dai-mojo-db/#result","text":"The DB scorer stores the score/prediction for each row from the given dataset in a file called results.csv in the main directory. java -Derrors=true -Dverbose=true -Dthreads=2 -Dcapacity=4000 -Xms2g -Xmx10g -Dpropertiesfilename=properties/DAIMojoRunner_DB.properties-Snowflake -jar build/libs/dai-mojo-db-2.30.jar H2O DAI Mojo Database Scoring v2.30 Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry EnvVariables not found in properties file, setting to default Property EnvVariables = false Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry IgnoreEmptyStrings not found in properties file, setting to default Property IgnoreEmptyStrings = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry IgnoreChecks not found in properties file, setting to default Property IgnoreChecks = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry IgnoreSQLChecks not found in properties file, setting to default Property IgnoreSQLChecks = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry ForceConnection not found in properties file, setting to default Property ForceConnection = False Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry ForceConnectionClass not found in properties file, setting to default Property ForceConnectionClass = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property ModelName = Mojo/pipeline.mojo Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLConnectionString = jdbc:snowflake://h20_ai_partner.snowflakecomputing.com/?warehouse=DEMO_WH&db=LENDINGCLUB&schema=PUBLIC Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLUser = VIYENGAR Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLPassword = MjMwN0xlZ2hvcm4= Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLPrompt not found in properties file, setting to default Property SQLPrompt = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLSelect = select id, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mnths, pub_rec, revol_bal, revol_util, total_acc from \"LENDINGCLUB\"; Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLWrite = CSV Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLWriteCSVFilename = results.csv Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLWriteBatch not found in properties file, setting to default Property SQLWriteBatch = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLWriteBatchSize not found in properties file, setting to default Property SQLWriteBatchSize = 100000 Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLKey = id Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLPrediction not found in properties file, setting to default Property SQLPrediction = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLPredictionLabel not found in properties file, setting to default Property SQLPredictionLabel = Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLFieldSeperator not found in properties file, setting to default Property SQLFieldSeperator = , Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry InternalFieldSeperator not found in properties file, setting to default Property InternalFieldSeperator = , Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry SQLWriteFieldSeperator not found in properties file, setting to default Property SQLWriteFieldSeperator = , Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLStartScript = snowflake/SnowflakeStart.sql Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Property SQLEndScript = snowflake/SnowflakeUpdate.sql Using properties file: properties/DAIMojoRunner_DB.properties-Snowflake Propertry Threshold not found in properties file, setting to default Property Threshold = Using model Mojo/pipeline.mojo TQ: 4000 2 Queue Capacity Starting 4000 Connection string used seperate SQL parameters Reader connected to database! {} Running SQL Script snowflake/SnowflakeStart.sql Script init completed Invoking SQL Script snowflake/SnowflakeStart.sql select count(*) from \"LENDINGCLUB\" COUNT(*) 39029 delete from \"RESULTS\" select count(*) from \"RESULTS\" COUNT(*) 0 SQL Script completed Reading ResultSet for select id, loan_amnt, term, int_rate, installment, emp_length, home_ownership, annual_inc, verification_status, addr_state, dti, delinq_2yrs, inq_last_6mnths, pub_rec, revol_bal, revol_util, total_acc from \"LENDINGCLUB\"; ID, LOAN_AMNT, TERM, INT_RATE, INSTALLMENT, EMP_LENGTH, HOME_OWNERSHIP, ANNUAL_INC, VERIFICATION_STATUS, ADDR_STATE, DTI, DELINQ_2YRS, INQ_LAST_6MNTHS, PUB_REC, REVOL_BAL, REVOL_UTIL, TOTAL_ACC RAW ResultSet: 1077501 RAW ResultSet: 5000 RAW ResultSet: 36 months RAW ResultSet: 10.65 RAW ResultSet: 162.87 RAW ResultSet: 10 RAW ResultSet: RENT RAW ResultSet: 24000 ... Script init completed Invoking SQL Script snowflake/SnowflakeUpdate.sql put file://results.csv @~/staged source target source_size target_size source_compression target_compression status encryption message results.csv results.csv.gz 1783130 814416 NONE GZIP UPLOADED ENCRYPTED copy into RESULTS from @~/staged file_format = ( type = CSV) remove @~/stages/results.csv.gz name result select count(*) from \"RESULTS\" COUNT(*) 39029 SQL Script completed Note Please create a results.csv file in the main directory (if it doesn't exist already). It is important for the execution of the above command to have a results.csv file in the main dir to save the data processed by the scorer as the code will not create the file itself if it doesn't exist .","title":"Result"},{"location":"dai-mojo-rest/","text":"REST Sever Scorer \u00b6 This scorer contains sources of a generic Java scorer implementation for Driverless AI MOJO scoring pipiline with Java runtime, based on SpringBoot. It is a server that accepts URL/Rest call and produces a score using a MOJO that was created from DAI. Downloading Instructions \u00b6 You can download the REST scorer from the downloads page Deployment \u00b6 To run the local scorer, you can directly run the executable jar: ``` java -Dai.h2o.mojos.runtime.license.file={LICENSE_FILE} -DModelDirectory={PATH_TO_MOJO_PIPELINE} -jar dai-mojo-restserver-{YOUR_CURRENT_VERSION}.jar ### URL Requests The general format is: curl \"127.0.0.1:8080/model?name=/tmp/pipeline.mojo&row=5000,36%20months,10.65,162.87,10,RENT,24000,VERIFIED%20-%20income,AZ,27.65,0,1,0,13648,83.7,0\" In the curl request above, I'm simply sending a row from the dataset for the restserver to process and give out `bad_loan.0` and `bad_loan.1` values for the input row Following requests are supported: - `/model` This is used to pass the row variable, the variable \"name\" is the model name to invoke. - `/modeljson` (for DAI) ``/modelh2ojson`` (for h2o) This is used to pass a row as a json object to the server for scoring. - `/modelfeatures` This lists the model column names for the model name passed as \"name\". - `/modelstats` This reports latency statistics on all models - `/modelreload` This forces all models to be reloaded on the next call and reinitializes the statistics. Only available in the DAIMojoRestServer4 distribution. - `/modelvars` This provides support to call a model using the feature names (as reported by modelfeatures) as variables in any order on the URL command. Example: ``` curl \"127.0.0.1:8080/modelvars?name=pipeline.mojo&loan_amnt=5000&term=36months&int_rate=10.65&installment=162.87&emp_length=10&home_ownership=RENT&annual_inc=24000&verification_status=VERIFIED-income&addr_state=AZ&dti=27.65&delinq_2yrs=0&inq_last_6mths=1&pub_rec=0&revol_bal=13648&revol_util=83.7&total_acc=0 ``` - `/modeltext` This calls the /model but returns only the result and not the target variable name, so it can be used in Excel for example. - `/modelsecure` Same call as /model, except using a basic authentication secured end point. - `/mli` This calls the klime MLI mojo, without scoring and returns the klime results. Note: using the explainability=true on the /model or /modelvars calls will return both the score and MLI results, in one REST call. - `/score` This enables the request to be forwarded to the Python HTTP server, this is used to get Shapley reason codes currently. Note very scalable but it works as a workaround. - `/batch` Uploads a file to be used for scoring, uses the same parameters are /model. Example: ``` curl -F \"file=@example.csv\" http://127.0.0.1:8080/batch?name=diabetes.mojo Add the parameter --spring.servlet.multipart.max-file-size=10MB ``--spring.servlet.multipart.max-request-size=10MB after the jar file to enable uploads over 1MB. See the folder Client for examples of sending the calls to the end point. ``` - `/upload` This like the batch call, except it returns a csv with the predictions per row. - `/qlik` This passes a csv style input row and returns a labeled row with the original data and predictions. Example: ``` curl \"127.0.0.1:8080/qlik?name=pipeline.mojo&verbose=true&row=5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0\" loan_amnt,term,int_rate,installment,emp_length,home_ownership,annual_inc,verification_status,addr_state,dti,delinq_2yrs,inq_last_6mths,pub_rec,revol_bal,revol_util,total_acc,bad_loan.0,bad_loan.1 5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0,0.2325363134421583,0.7674636865578417 ``` - `/model2JSON` Like `/model` but returns just the prediction as separate JSON elements for easier parsing by some tools. ### URL Variables - `name` Absolute path to the model name on the server, default /tmp/pipeline.mojo - `row` This is the row data to be scored, add %20 for spaces as per the http protocol requirements. - `type` This is the model type, default 2. - `verbose` Boolean to enable (true) extra logging on the server standard out, default false. - `explainability` Boolean to indicate if MLI results should be returned for this request. Default false. ## Response Output The server will log the latency in nanoseconds of the scoring function to the processes standard out. The client issuing the request (curl, python, java etc) will receive a JSON response, with one key called `result`. If the result contains multiple lines, then the JSON response is a list within the bodies response, for example the response from a `/modelstats` request. ## Default Port The application uses port 8080, if a different port is required add -Dserver.port=xxxx before the -jar on the command line. java -Dai.h2o.mojos.runtime.license.file=license.sig -DServer.port=8989 -jar dai-mojo-restserver.jar ``` Any of the Spring Application properties can be overridden in this way. Security \u00b6 The example uses Basic Authentication on the /modelsecure end point only. The parameter -DSecureEndPoint=\"/**\" can be added to the command line, to secure all the end points. To access a secure end point, the username h2o password h2o123 must be passed. If you are using a web browser, a logon panel will prompt you. If using a Rest API call, then pass a base64 encoded username password. I use PostMan as a tool to setup and test the API call. Command Line Parameters \u00b6 The following options are also available when starting the Server. -DModelDirectory= This is the full path to the location of the Mojo's to serve. -DSecureEndPoints= The url that will trigger a security challenge (Basic HTTP Auth). Defaults to /modelsecure so any request requires authentication. Example: http://127.0.0.1:8443/modelsecure?name=pipeline.mojo&verbose=true&row=5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0 Default user h2o (-DRestUser=) and password h2o123 (-DRestPass=) for secure requests. -DScoreHostIP IP address to remote Python HTTP server to forward specific requests (/score) -DScoreLog Option to log into a separate log file for audit and control all scoring and MLI requests and results. Default False. -DExtendedOutput Option to write more verbose JSON result string for the result from the call. Default False. -DDetectShift If the row being scored should be compared (number fields only) to see if the data is outside of the range the model was trained on. Default False. Requires the Experiment Summary zip file from DAI to be saved into the mojo directory with the same name as the mojo for example a DAI mojo called pipeline.mojo would have the experiment summary saved as pipeline.mojo.experiment_summary (with NO zip extension). -DDetectShiftTolerance= Float representing a percentage over the training data that shift is allowed before alerting in the logs and via JMX. Default 0. -DDetectShiftEnforce= Boolean to indicate, if the scoring row numeric values are outside of the range trained on to NOT score but return no result. Questions \u00b6 Feel free to email me and I will try to help egudgion@h2o.ai","title":"REST Sever Scorer"},{"location":"dai-mojo-rest/#rest-sever-scorer","text":"This scorer contains sources of a generic Java scorer implementation for Driverless AI MOJO scoring pipiline with Java runtime, based on SpringBoot. It is a server that accepts URL/Rest call and produces a score using a MOJO that was created from DAI.","title":"REST Sever Scorer"},{"location":"dai-mojo-rest/#downloading-instructions","text":"You can download the REST scorer from the downloads page","title":"Downloading Instructions"},{"location":"dai-mojo-rest/#deployment","text":"To run the local scorer, you can directly run the executable jar: ``` java -Dai.h2o.mojos.runtime.license.file={LICENSE_FILE} -DModelDirectory={PATH_TO_MOJO_PIPELINE} -jar dai-mojo-restserver-{YOUR_CURRENT_VERSION}.jar ### URL Requests The general format is: curl \"127.0.0.1:8080/model?name=/tmp/pipeline.mojo&row=5000,36%20months,10.65,162.87,10,RENT,24000,VERIFIED%20-%20income,AZ,27.65,0,1,0,13648,83.7,0\" In the curl request above, I'm simply sending a row from the dataset for the restserver to process and give out `bad_loan.0` and `bad_loan.1` values for the input row Following requests are supported: - `/model` This is used to pass the row variable, the variable \"name\" is the model name to invoke. - `/modeljson` (for DAI) ``/modelh2ojson`` (for h2o) This is used to pass a row as a json object to the server for scoring. - `/modelfeatures` This lists the model column names for the model name passed as \"name\". - `/modelstats` This reports latency statistics on all models - `/modelreload` This forces all models to be reloaded on the next call and reinitializes the statistics. Only available in the DAIMojoRestServer4 distribution. - `/modelvars` This provides support to call a model using the feature names (as reported by modelfeatures) as variables in any order on the URL command. Example: ``` curl \"127.0.0.1:8080/modelvars?name=pipeline.mojo&loan_amnt=5000&term=36months&int_rate=10.65&installment=162.87&emp_length=10&home_ownership=RENT&annual_inc=24000&verification_status=VERIFIED-income&addr_state=AZ&dti=27.65&delinq_2yrs=0&inq_last_6mths=1&pub_rec=0&revol_bal=13648&revol_util=83.7&total_acc=0 ``` - `/modeltext` This calls the /model but returns only the result and not the target variable name, so it can be used in Excel for example. - `/modelsecure` Same call as /model, except using a basic authentication secured end point. - `/mli` This calls the klime MLI mojo, without scoring and returns the klime results. Note: using the explainability=true on the /model or /modelvars calls will return both the score and MLI results, in one REST call. - `/score` This enables the request to be forwarded to the Python HTTP server, this is used to get Shapley reason codes currently. Note very scalable but it works as a workaround. - `/batch` Uploads a file to be used for scoring, uses the same parameters are /model. Example: ``` curl -F \"file=@example.csv\" http://127.0.0.1:8080/batch?name=diabetes.mojo Add the parameter --spring.servlet.multipart.max-file-size=10MB ``--spring.servlet.multipart.max-request-size=10MB after the jar file to enable uploads over 1MB. See the folder Client for examples of sending the calls to the end point. ``` - `/upload` This like the batch call, except it returns a csv with the predictions per row. - `/qlik` This passes a csv style input row and returns a labeled row with the original data and predictions. Example: ``` curl \"127.0.0.1:8080/qlik?name=pipeline.mojo&verbose=true&row=5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0\" loan_amnt,term,int_rate,installment,emp_length,home_ownership,annual_inc,verification_status,addr_state,dti,delinq_2yrs,inq_last_6mths,pub_rec,revol_bal,revol_util,total_acc,bad_loan.0,bad_loan.1 5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0,0.2325363134421583,0.7674636865578417 ``` - `/model2JSON` Like `/model` but returns just the prediction as separate JSON elements for easier parsing by some tools. ### URL Variables - `name` Absolute path to the model name on the server, default /tmp/pipeline.mojo - `row` This is the row data to be scored, add %20 for spaces as per the http protocol requirements. - `type` This is the model type, default 2. - `verbose` Boolean to enable (true) extra logging on the server standard out, default false. - `explainability` Boolean to indicate if MLI results should be returned for this request. Default false. ## Response Output The server will log the latency in nanoseconds of the scoring function to the processes standard out. The client issuing the request (curl, python, java etc) will receive a JSON response, with one key called `result`. If the result contains multiple lines, then the JSON response is a list within the bodies response, for example the response from a `/modelstats` request. ## Default Port The application uses port 8080, if a different port is required add -Dserver.port=xxxx before the -jar on the command line. java -Dai.h2o.mojos.runtime.license.file=license.sig -DServer.port=8989 -jar dai-mojo-restserver.jar ``` Any of the Spring Application properties can be overridden in this way.","title":"Deployment"},{"location":"dai-mojo-rest/#security","text":"The example uses Basic Authentication on the /modelsecure end point only. The parameter -DSecureEndPoint=\"/**\" can be added to the command line, to secure all the end points. To access a secure end point, the username h2o password h2o123 must be passed. If you are using a web browser, a logon panel will prompt you. If using a Rest API call, then pass a base64 encoded username password. I use PostMan as a tool to setup and test the API call.","title":"Security"},{"location":"dai-mojo-rest/#command-line-parameters","text":"The following options are also available when starting the Server. -DModelDirectory= This is the full path to the location of the Mojo's to serve. -DSecureEndPoints= The url that will trigger a security challenge (Basic HTTP Auth). Defaults to /modelsecure so any request requires authentication. Example: http://127.0.0.1:8443/modelsecure?name=pipeline.mojo&verbose=true&row=5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0 Default user h2o (-DRestUser=) and password h2o123 (-DRestPass=) for secure requests. -DScoreHostIP IP address to remote Python HTTP server to forward specific requests (/score) -DScoreLog Option to log into a separate log file for audit and control all scoring and MLI requests and results. Default False. -DExtendedOutput Option to write more verbose JSON result string for the result from the call. Default False. -DDetectShift If the row being scored should be compared (number fields only) to see if the data is outside of the range the model was trained on. Default False. Requires the Experiment Summary zip file from DAI to be saved into the mojo directory with the same name as the mojo for example a DAI mojo called pipeline.mojo would have the experiment summary saved as pipeline.mojo.experiment_summary (with NO zip extension). -DDetectShiftTolerance= Float representing a percentage over the training data that shift is allowed before alerting in the logs and via JMX. Default 0. -DDetectShiftEnforce= Boolean to indicate, if the scoring row numeric values are outside of the range trained on to NOT score but return no result.","title":"Command Line Parameters"},{"location":"dai-mojo-rest/#questions","text":"Feel free to email me and I will try to help egudgion@h2o.ai","title":"Questions"},{"location":"gcp/","text":"Driverless AI Deployment Template for Google Cloud Run \u00b6 This template extends the implementation of Local Rest Scorer here and scores data using Driverless AI Mojo pipeline with Java Runtime. The docker image that is built by this project can be pushed to gcr.io and used for scoring Driverless AI Mojos in Google Cloud Run . The user needs to provide Driverless AI license key and model's pipeline.mojo file as input for scoring. The versions of software used to create the template like mojo runtime are listed here . Building \u00b6 Since there is a direct dependency on the separate, above mentioned project local-rest-scorer it is best to build this project from the root directory. Make sure you are in the working directory dai-deployment-templates . Typing pwd in the terminal shell should have a similar output to /my/path/to/dai-deployment-templates . Run the following command: ./gradlew build and the docker image required for Google Cloud Run will be in the directory gcp-cloud-run/build : /path/to/dai-deployment-templates/gcp-cloud-run/build/jib-image.tar Load the resulting jib-image.tar file to docker: docker load < /path/to/dai-deployment-templates/gcp-cloud-run/build/jib-image.tar Follow the steps explained here in Google Documentation: https://cloud.google.com/run/docs/building/containers, to push the container to gcr.io Deploying \u00b6 To deploy the container follow the steps in Google Documentation here: https://cloud.google.com/run/docs/deploying There is one requirement for the container. You MUST include the following environment variables: MOJO_GCS_PATH = gs://path/to/pipeline.mojo LICENSE_GCS_PATH = gs://path/to/driverless/ai/license.sig Scoring \u00b6 On a successful deployment to Google Cloud Run, you will be provided an endpoint that can be scored against. The api for scoring is the same as the Local Rest Scorer To access api information: https://<google provided endpoint>/swagger-ui.html To score the model: https://<google provided endpoint>/model/score Sample curl request: curl -X POST -H \"Content-Type: application/json\" \\ -d '{\"includeFieldsInOutput\":null,\"noFieldNamesInOutput\":null,\"idField\":null, \\ \"fields\":[\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\",\"PAY_1\",\"PAY_2\", \\ \"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\", \\ \"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\", \\ \"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"], \\ \"rows\":[[\"0\",\"text\",\"text\",\"text\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"]]}' \\ https://<google provided endpoint>/model/score","title":"Google Cloud Run Scorer"},{"location":"gcp/#driverless-ai-deployment-template-for-google-cloud-run","text":"This template extends the implementation of Local Rest Scorer here and scores data using Driverless AI Mojo pipeline with Java Runtime. The docker image that is built by this project can be pushed to gcr.io and used for scoring Driverless AI Mojos in Google Cloud Run . The user needs to provide Driverless AI license key and model's pipeline.mojo file as input for scoring. The versions of software used to create the template like mojo runtime are listed here .","title":"Driverless AI Deployment Template for Google Cloud Run"},{"location":"gcp/#building","text":"Since there is a direct dependency on the separate, above mentioned project local-rest-scorer it is best to build this project from the root directory. Make sure you are in the working directory dai-deployment-templates . Typing pwd in the terminal shell should have a similar output to /my/path/to/dai-deployment-templates . Run the following command: ./gradlew build and the docker image required for Google Cloud Run will be in the directory gcp-cloud-run/build : /path/to/dai-deployment-templates/gcp-cloud-run/build/jib-image.tar Load the resulting jib-image.tar file to docker: docker load < /path/to/dai-deployment-templates/gcp-cloud-run/build/jib-image.tar Follow the steps explained here in Google Documentation: https://cloud.google.com/run/docs/building/containers, to push the container to gcr.io","title":"Building"},{"location":"gcp/#deploying","text":"To deploy the container follow the steps in Google Documentation here: https://cloud.google.com/run/docs/deploying There is one requirement for the container. You MUST include the following environment variables: MOJO_GCS_PATH = gs://path/to/pipeline.mojo LICENSE_GCS_PATH = gs://path/to/driverless/ai/license.sig","title":"Deploying"},{"location":"gcp/#scoring","text":"On a successful deployment to Google Cloud Run, you will be provided an endpoint that can be scored against. The api for scoring is the same as the Local Rest Scorer To access api information: https://<google provided endpoint>/swagger-ui.html To score the model: https://<google provided endpoint>/model/score Sample curl request: curl -X POST -H \"Content-Type: application/json\" \\ -d '{\"includeFieldsInOutput\":null,\"noFieldNamesInOutput\":null,\"idField\":null, \\ \"fields\":[\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\",\"PAY_1\",\"PAY_2\", \\ \"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\", \\ \"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\", \\ \"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"], \\ \"rows\":[[\"0\",\"text\",\"text\",\"text\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"]]}' \\ https://<google provided endpoint>/model/score","title":"Scoring"},{"location":"local-rest-scorer/","text":"Driverless AI Deployment Template for Local SpringBoot Scorer \u00b6 This template contains sources of a generic Java scorer implementation for Driverless AI MOJO scoring pipiline with Java runtime , based on SpringBoot and its Docker image. This scorer can be used to do Real-time scoring on single or multiple rows. The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring. The versions of software used to create the template like mojo runtime are listed here . Building \u00b6 The code of the local SpringBoot scorer is a gradle project build as usual by ./gradlew build . The resulting executable jar is located in the build/libs folder. Running \u00b6 To run the local scorer, you can either use bootRun gradle task or run directly the executable jar: java -Dmojo.path={PATH_TO_MOJO_PIPELINE} -jar build/libs/local-rest-scorer-{YOUR_CURRENT_VERSION}-boot.jar Score JSON Request \u00b6 To test the endpoint, send a request to http://localhost:8080 as follows: curl \\ -X POST \\ -H \"Content-Type: application/json\" \\ -d @test.json http://localhost:8080/model/score This expects a file test.json with the actual scoring request payload. If you are using the mojo trained in test/data/iris.csv as suggested above, you should be able to use the following json payload: { \"fields\": [ \"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\" ], \"includeFieldsInOutput\": [ \"sepal_len\" ], \"rows\": [ [ \"1.0\", \"1.0\", \"2.2\", \"3.5\" ], [ \"3.0\", \"10.0\", \"2.2\", \"3.5\" ], [ \"4.0\", \"100.0\", \"2.2\", \"3.5\" ] ] } The expected response should follow this structure, but the actual values may differ: { \"id\": \"a12e7390-b8ac-406a-ade9-0d5ea4b63ea9\", \"fields\": [ \"sepal_len\", \"class.Iris-setosa\", \"class.Iris-versicolor\", \"class.Iris-virginica\" ], \"score\": [ [ \"1.0\", \"0.6240277982943945\", \"0.045458571508101536\", \"0.330513630197504\" ], [ \"3.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ], [ \"4.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ] ] } Note that including the fields in the response can be disabled by setting noFieldNamesInOutput to true in the input request. Score CSV File \u00b6 Alternatively, you can score an existing file on the local filesystem using GET request to the same endpoint: curl -X GET http://localhost:8080/model/score/?file=/tmp/test.csv This expects a CSV file /tmp/test.csv to exist on the machine where the scorer runs (i.e., it is not send to it over HTTP). Model ID \u00b6 You can get the UUID of the loaded pipeline by calling the following: $ curl http://localhost:8080/model/id Which should return the UUID of the loaded mojo model. Alternatively, the scorer log contains the UUID as well in the form: Mojo pipeline successfully loaded (a12e7390-b8ac-406a-ade9-0d5ea4b63ea9). The hex string in parenthesis is the UUID of you mojo pipeline. Get Example Request \u00b6 The scorer can also provide an example request that would pass all validations. This way, users can quickly get an example scoring request to send to the scorer to test it. This request can be further filled with meaningful input values. curl -X GET http://localhost:8080/model/sample_request The resulting JSON is a valid input for the POST /model/score request. API Inspection \u00b6 You can use SpringFox endpoints that allow both programmatic and manual inspection of the API: Swagger JSON representation for programmatic access: http://localhost:8080/v2/api-docs. The UI for manual API inspection: http://localhost:8080/swagger-ui.html. Docker Image \u00b6 Docker image for this REST scorer is built using Jib . Build Image \u00b6 Generation of this Docker image is plugged into the build process of this project. Run the following command in the root project directory to run the build process. ./gradlew :local-rest-scorer:jibDockerBuild Verify that the Docker image was created, and take note of the version created. docker images --format \"{{.Repository}} \\t {{.Tag}}\" | grep \"h2oai/rest-scorer\" Run Container \u00b6 Note: Replace <version> with the version of the image you found from the previous step. docker run \\ --name rest-scorer \\ -v /path/to/local/pipeline.mojo:/mojos/pipeline.mojo:ro \\ -v /path/to/local/license.sig:/secrets/license.sig:ro \\ -p 8080:8080 \\ h2oai/rest-scorer:<version> Notice how the desired MOJO was mounted to the container: -v /path/to/local/pipeline.mojo:/mojos/pipeline.mojo:ro Notice how your H2O.ai DriverlessAI license was mounted to the container: -v /path/to/local/license.sig:/secrets/license.sig:ro Alternatively, you could pass in your license as an environment variable: First, export your license key. read -s DRIVERLESS_AI_LICENSE_KEY < /path/to/local/license.sig export DRIVERLESS_AI_LICENSE_KEY Note: Option -s , above, hides the echoing of your license so that its content is not written to logs. Now start a container. docker run \\ --name rest-scorer \\ -v /path/to/local/pipeline.mojo:/mojos/pipeline.mojo:ro \\ -e DRIVERLESS_AI_LICENSE_KEY \\ -p 8080:8080 \\ h2oai/rest-scorer:<version> See section Running above for information on how to score requests.","title":"Local Rest Server Scorer"},{"location":"local-rest-scorer/#driverless-ai-deployment-template-for-local-springboot-scorer","text":"This template contains sources of a generic Java scorer implementation for Driverless AI MOJO scoring pipiline with Java runtime , based on SpringBoot and its Docker image. This scorer can be used to do Real-time scoring on single or multiple rows. The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring. The versions of software used to create the template like mojo runtime are listed here .","title":"Driverless AI Deployment Template for Local SpringBoot Scorer"},{"location":"local-rest-scorer/#building","text":"The code of the local SpringBoot scorer is a gradle project build as usual by ./gradlew build . The resulting executable jar is located in the build/libs folder.","title":"Building"},{"location":"local-rest-scorer/#running","text":"To run the local scorer, you can either use bootRun gradle task or run directly the executable jar: java -Dmojo.path={PATH_TO_MOJO_PIPELINE} -jar build/libs/local-rest-scorer-{YOUR_CURRENT_VERSION}-boot.jar","title":"Running"},{"location":"local-rest-scorer/#score-json-request","text":"To test the endpoint, send a request to http://localhost:8080 as follows: curl \\ -X POST \\ -H \"Content-Type: application/json\" \\ -d @test.json http://localhost:8080/model/score This expects a file test.json with the actual scoring request payload. If you are using the mojo trained in test/data/iris.csv as suggested above, you should be able to use the following json payload: { \"fields\": [ \"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\" ], \"includeFieldsInOutput\": [ \"sepal_len\" ], \"rows\": [ [ \"1.0\", \"1.0\", \"2.2\", \"3.5\" ], [ \"3.0\", \"10.0\", \"2.2\", \"3.5\" ], [ \"4.0\", \"100.0\", \"2.2\", \"3.5\" ] ] } The expected response should follow this structure, but the actual values may differ: { \"id\": \"a12e7390-b8ac-406a-ade9-0d5ea4b63ea9\", \"fields\": [ \"sepal_len\", \"class.Iris-setosa\", \"class.Iris-versicolor\", \"class.Iris-virginica\" ], \"score\": [ [ \"1.0\", \"0.6240277982943945\", \"0.045458571508101536\", \"0.330513630197504\" ], [ \"3.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ], [ \"4.0\", \"0.7209441819603676\", \"0.06299909138586585\", \"0.21605672665376663\" ] ] } Note that including the fields in the response can be disabled by setting noFieldNamesInOutput to true in the input request.","title":"Score JSON Request"},{"location":"local-rest-scorer/#score-csv-file","text":"Alternatively, you can score an existing file on the local filesystem using GET request to the same endpoint: curl -X GET http://localhost:8080/model/score/?file=/tmp/test.csv This expects a CSV file /tmp/test.csv to exist on the machine where the scorer runs (i.e., it is not send to it over HTTP).","title":"Score CSV File"},{"location":"local-rest-scorer/#model-id","text":"You can get the UUID of the loaded pipeline by calling the following: $ curl http://localhost:8080/model/id Which should return the UUID of the loaded mojo model. Alternatively, the scorer log contains the UUID as well in the form: Mojo pipeline successfully loaded (a12e7390-b8ac-406a-ade9-0d5ea4b63ea9). The hex string in parenthesis is the UUID of you mojo pipeline.","title":"Model ID"},{"location":"local-rest-scorer/#get-example-request","text":"The scorer can also provide an example request that would pass all validations. This way, users can quickly get an example scoring request to send to the scorer to test it. This request can be further filled with meaningful input values. curl -X GET http://localhost:8080/model/sample_request The resulting JSON is a valid input for the POST /model/score request.","title":"Get Example Request"},{"location":"local-rest-scorer/#api-inspection","text":"You can use SpringFox endpoints that allow both programmatic and manual inspection of the API: Swagger JSON representation for programmatic access: http://localhost:8080/v2/api-docs. The UI for manual API inspection: http://localhost:8080/swagger-ui.html.","title":"API Inspection"},{"location":"local-rest-scorer/#docker-image","text":"Docker image for this REST scorer is built using Jib .","title":"Docker Image"},{"location":"local-rest-scorer/#build-image","text":"Generation of this Docker image is plugged into the build process of this project. Run the following command in the root project directory to run the build process. ./gradlew :local-rest-scorer:jibDockerBuild Verify that the Docker image was created, and take note of the version created. docker images --format \"{{.Repository}} \\t {{.Tag}}\" | grep \"h2oai/rest-scorer\"","title":"Build Image"},{"location":"local-rest-scorer/#run-container","text":"Note: Replace <version> with the version of the image you found from the previous step. docker run \\ --name rest-scorer \\ -v /path/to/local/pipeline.mojo:/mojos/pipeline.mojo:ro \\ -v /path/to/local/license.sig:/secrets/license.sig:ro \\ -p 8080:8080 \\ h2oai/rest-scorer:<version> Notice how the desired MOJO was mounted to the container: -v /path/to/local/pipeline.mojo:/mojos/pipeline.mojo:ro Notice how your H2O.ai DriverlessAI license was mounted to the container: -v /path/to/local/license.sig:/secrets/license.sig:ro Alternatively, you could pass in your license as an environment variable: First, export your license key. read -s DRIVERLESS_AI_LICENSE_KEY < /path/to/local/license.sig export DRIVERLESS_AI_LICENSE_KEY Note: Option -s , above, hides the echoing of your license so that its content is not written to logs. Now start a container. docker run \\ --name rest-scorer \\ -v /path/to/local/pipeline.mojo:/mojos/pipeline.mojo:ro \\ -e DRIVERLESS_AI_LICENSE_KEY \\ -p 8080:8080 \\ h2oai/rest-scorer:<version> See section Running above for information on how to score requests.","title":"Run Container"},{"location":"sql-jdbc-scorer/","text":"Driverless AI Deployment Template for Local Rest SQL Scorer \u00b6 This template contains an implementation of generic Java implementation for scoring Driverless AI Mojos( java runtime) against a SQL database. The application runs as a restful service and receives requests that include a SQL query and additional, appropriate parameters for scoring the table that results from the SQL query, and writing the preditions back to the database. The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring. The versions of software used to create the template like mojo runtime are listed here . Implementation \u00b6 The implementation leverages Spark APIs to handle data ingest/export via JDBC and Sparkling Water API's to manage distributed scoring in a Spark Session. This decision was made in order to allow for larger queries as Spark can manage data partitions very cleanly and avoid OOM errors. Building \u00b6 From the root project of this repository. You can run the following: ./gradlew build This will build the entire project and resultant jar file required for running the application will be available in the directory: build/libs Running \u00b6 In order to run the application you will need 4 files in addition to the jar file: Driverless AI License file - typically: license.sig Driverless AI Mojo - typically: pipeline.mojo JDBC configuration file - typically: jdbc.conf , example found here: jdbc.conf Note: the configuration file contains the expected JDBC driver class name org.postgresql.Driver which applies to the example below. JDBC Driver jar - the below example uses postgresql-42.2.5.jar , but this should be what ever JDBC driver jar is used by the database in question. These files will be added to the JVM via system properties using -D flag. Example run command: java -cp sql-jdbc-scorer-1.0.6-SNAPSHOT.jar \\ -Dmojo.path=pipeline.mojo \\ -Djdbc.config.path=jdbc.conf \\ -Dloader.path=postgresql-42.2.5.jar \\ -Dloader.main=ai.h2o.mojos.deploy.sql.db.ScorerApplication \\ org.springframework.boot.loader.PropertiesLauncher Score Request \u00b6 There are 2 methods of scoring. Both with the same end result: GET - takes input parameters in GET query and scores resultant dataset to the configured database POST - takes input json payload and scores resultant dataset to the configured database Parameter inputs for both methods: sqlQuery : String representation of a SQL Query. Ex. SELECT * FROM myTable outputTable : String representation of destination table for scorer to write to idColumn : numeric unique key for scorer to use for proper data partitioning. also can be used as key to join on with original table Note : this can be an empty string \"\"\" , but if no idColumn is provided there is a possibility that the application will run out of memory if a very large query is provided. Additionally, the resultant table will only contain the prediction columns making it hard to reference against the original table. saveMethod : one of [preview, append, overwrite, ignore, error] each with different behavior: preview: does not write to the database, only gives preview of resultant scored dataset in response All others are clearly documented as part of Spark API: here GET \u00b6 example query: http://localhost:8080/model/score?sqlQuery=%22SELECT%20*%20FROM%20creditcardtrain%22&outputTable=%22helloworld%22&idColumn=%22id%22 POST \u00b6 example query: curl -X POST -H \"Content-Type: application/json\" \\ -d '{\"query\": \"SELECT * FROM creditcardtrain LIMIT 10\", \"idColumn\": \"id\", \"saveMethod\": \"overwrite\", \"outputTable\": \"helloworld\"}' \\ http://localhost:8080/model/score Model ID \u00b6 You can get the UUID of the loaded pipeline by calling the following: $ curl http://localhost:8080/model/id API Inspection \u00b6 You can use SpringFox endpoints that allow both programmatic and manual inspection of the API: Swagger JSON representation for programmatic access: http://localhost:8080/v2/api-docs. The UI for manual API inspection: http://localhost:8080/swagger-ui.html.","title":"SQL JDBC Scorer"},{"location":"sql-jdbc-scorer/#driverless-ai-deployment-template-for-local-rest-sql-scorer","text":"This template contains an implementation of generic Java implementation for scoring Driverless AI Mojos( java runtime) against a SQL database. The application runs as a restful service and receives requests that include a SQL query and additional, appropriate parameters for scoring the table that results from the SQL query, and writing the preditions back to the database. The user needs to provide Driverless AI license key and model's pipeline.mojo file for scoring. The versions of software used to create the template like mojo runtime are listed here .","title":"Driverless AI Deployment Template for Local Rest SQL Scorer"},{"location":"sql-jdbc-scorer/#implementation","text":"The implementation leverages Spark APIs to handle data ingest/export via JDBC and Sparkling Water API's to manage distributed scoring in a Spark Session. This decision was made in order to allow for larger queries as Spark can manage data partitions very cleanly and avoid OOM errors.","title":"Implementation"},{"location":"sql-jdbc-scorer/#building","text":"From the root project of this repository. You can run the following: ./gradlew build This will build the entire project and resultant jar file required for running the application will be available in the directory: build/libs","title":"Building"},{"location":"sql-jdbc-scorer/#running","text":"In order to run the application you will need 4 files in addition to the jar file: Driverless AI License file - typically: license.sig Driverless AI Mojo - typically: pipeline.mojo JDBC configuration file - typically: jdbc.conf , example found here: jdbc.conf Note: the configuration file contains the expected JDBC driver class name org.postgresql.Driver which applies to the example below. JDBC Driver jar - the below example uses postgresql-42.2.5.jar , but this should be what ever JDBC driver jar is used by the database in question. These files will be added to the JVM via system properties using -D flag. Example run command: java -cp sql-jdbc-scorer-1.0.6-SNAPSHOT.jar \\ -Dmojo.path=pipeline.mojo \\ -Djdbc.config.path=jdbc.conf \\ -Dloader.path=postgresql-42.2.5.jar \\ -Dloader.main=ai.h2o.mojos.deploy.sql.db.ScorerApplication \\ org.springframework.boot.loader.PropertiesLauncher","title":"Running"},{"location":"sql-jdbc-scorer/#score-request","text":"There are 2 methods of scoring. Both with the same end result: GET - takes input parameters in GET query and scores resultant dataset to the configured database POST - takes input json payload and scores resultant dataset to the configured database Parameter inputs for both methods: sqlQuery : String representation of a SQL Query. Ex. SELECT * FROM myTable outputTable : String representation of destination table for scorer to write to idColumn : numeric unique key for scorer to use for proper data partitioning. also can be used as key to join on with original table Note : this can be an empty string \"\"\" , but if no idColumn is provided there is a possibility that the application will run out of memory if a very large query is provided. Additionally, the resultant table will only contain the prediction columns making it hard to reference against the original table. saveMethod : one of [preview, append, overwrite, ignore, error] each with different behavior: preview: does not write to the database, only gives preview of resultant scored dataset in response All others are clearly documented as part of Spark API: here","title":"Score Request"},{"location":"sql-jdbc-scorer/#get","text":"example query: http://localhost:8080/model/score?sqlQuery=%22SELECT%20*%20FROM%20creditcardtrain%22&outputTable=%22helloworld%22&idColumn=%22id%22","title":"GET"},{"location":"sql-jdbc-scorer/#post","text":"example query: curl -X POST -H \"Content-Type: application/json\" \\ -d '{\"query\": \"SELECT * FROM creditcardtrain LIMIT 10\", \"idColumn\": \"id\", \"saveMethod\": \"overwrite\", \"outputTable\": \"helloworld\"}' \\ http://localhost:8080/model/score","title":"POST"},{"location":"sql-jdbc-scorer/#model-id","text":"You can get the UUID of the loaded pipeline by calling the following: $ curl http://localhost:8080/model/id","title":"Model ID"},{"location":"sql-jdbc-scorer/#api-inspection","text":"You can use SpringFox endpoints that allow both programmatic and manual inspection of the API: Swagger JSON representation for programmatic access: http://localhost:8080/v2/api-docs. The UI for manual API inspection: http://localhost:8080/swagger-ui.html.","title":"API Inspection"}]}